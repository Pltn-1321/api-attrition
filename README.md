# API Attrition

API FastAPI pour la pr√©diction d'attrition des employ√©s avec machine learning.

## Structure du Projet

```
api-attrition/
‚îú‚îÄ‚îÄ api/                        # Sch√©mas et mod√®les de l'API
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py              # Sch√©mas Pydantic pour validation
‚îú‚îÄ‚îÄ data/                       # Donn√©es d'entra√Ænement et de test
‚îÇ   ‚îî‚îÄ‚îÄ export-api/
‚îÇ       ‚îî‚îÄ‚îÄ test_employees.csv  # Dataset des employ√©s (294 lignes)
‚îú‚îÄ‚îÄ database/                   # Configuration base de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ models.py               # Mod√®les ORM
‚îÇ   ‚îî‚îÄ‚îÄ import_data.py          # Script d'import CSV vers PostgreSQL
‚îú‚îÄ‚îÄ streamlit_app/              # Interface web Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ app.py                  # Page d'accueil
‚îÇ   ‚îú‚îÄ‚îÄ pages/                  # Pages multi-pages
‚îÇ   ‚îú‚îÄ‚îÄ utils/                  # Client API et composants UI
‚îÇ   ‚îú‚îÄ‚îÄ tests/                  # Tests unitaires et fonctionnels
‚îÇ   ‚îú‚îÄ‚îÄ .streamlit/             # Configuration et th√®me
‚îÇ   ‚îú‚îÄ‚îÄ DOCUMENTATION.md        # Documentation p√©dagogique compl√®te
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt        # D√©pendances Python
‚îú‚îÄ‚îÄ models/                     # Mod√®les ML (√† venir)
‚îú‚îÄ‚îÄ scripts/                    # Scripts utilitaires
‚îú‚îÄ‚îÄ tests/                      # Tests unitaires API
‚îú‚îÄ‚îÄ .github/workflows/          # CI/CD GitHub Actions
‚îú‚îÄ‚îÄ main.py                     # Application FastAPI principale
‚îú‚îÄ‚îÄ docker-compose.yml          # Configuration Docker (PostgreSQL)
‚îú‚îÄ‚îÄ pyproject.toml              # Configuration uv et d√©pendances
‚îî‚îÄ‚îÄ README.md                   # Documentation du projet
```

## Branches Git

- `main` (production)
- `dev` (d√©veloppement - branche de base)

## Technologies

- **FastAPI**: Framework API Python
- **SQLAlchemy**: ORM pour base de donn√©es
- **PostgreSQL**: Base de donn√©es
- **Scikit-learn**: Machine Learning
- **Pydantic**: Validation des donn√©es
- **Uvicorn**: Serveur ASGI

## Pr√©requis

- **Docker** et **Docker Compose** install√©s ([T√©l√©charger Docker Desktop](https://www.docker.com/products/docker-desktop))
- **Python 3.11+**
- **uv** - Gestionnaire de paquets Python moderne ([Installation uv](https://docs.astral.sh/uv/))

## Installation

```bash
# Cloner le d√©p√¥t
git clone git@github.com:Pltn-1321/api-attrition.git
cd api-attrition

# Initialiser le projet avec uv
uv init --no-readme

# Installer les d√©pendances
uv add pandas sqlalchemy psycopg2-binary fastapi uvicorn
```

## Installation de la Base de Donn√©es

### √âtape 1 : D√©marrer PostgreSQL avec Docker

Le projet utilise Docker Compose pour g√©rer PostgreSQL. La configuration se trouve dans `docker-compose.yml`.

```bash
# D√©marrer PostgreSQL
docker-compose up -d

# V√©rifier que le container tourne
docker ps
```

Vous devriez voir le container `attrition_db` (PostgreSQL sur le port 5432).

### √âtape 2 : Importer les donn√©es dans PostgreSQL

Le script `database/import_data.py` charge automatiquement le fichier CSV dans PostgreSQL :

```bash
# Importer les donn√©es
uv run database/import_data.py
```

R√©sultat attendu :
```
‚úÖ 294 employ√©s import√©s dans PostgreSQL
```

### √âtape 3 : V√©rifier l'import

```bash
# Compter le nombre d'employ√©s
docker exec attrition_db psql -U attrition_user -d attrition_db -c "SELECT COUNT(*) FROM employees;"

# Afficher quelques lignes
docker exec attrition_db psql -U attrition_user -d attrition_db -c "SELECT * FROM employees LIMIT 5;"
```

## Acc√©der √† la Base de Donn√©es (optionnel)

### Ligne de commande

```bash
# Acc√®s direct au terminal PostgreSQL
docker exec attrition_db psql -U attrition_user -d attrition_db

# Lister les tables
docker exec attrition_db psql -U attrition_user -d attrition_db -c "\dt"

# Ex√©cuter une requ√™te SQL
docker exec attrition_db psql -U attrition_user -d attrition_db -c "SELECT * FROM employees WHERE age > 30 LIMIT 10;"
```

### Clients Desktop (optionnel)

Vous pouvez utiliser des clients PostgreSQL comme :
- **TablePlus** (Mac/Windows) : https://tableplus.com
- **DBeaver** (Gratuit, multi-plateforme) : https://dbeaver.io

**Param√®tres de connexion** :
- Host : `localhost`
- Port : `5432`
- Database : `attrition_db`
- Username : `attrition_user`
- Password : `attrition_pass`

## Commandes Docker Utiles

```bash
# Arr√™ter PostgreSQL
docker-compose down

# Red√©marrer PostgreSQL
docker-compose up -d

# Voir les logs
docker logs attrition_db

# R√©initialiser la base de donn√©es (‚ö†Ô∏è supprime toutes les donn√©es)
docker-compose down -v
docker-compose up -d
uv run database/import_data.py
```

## Lancement de l'Application

### Option 1 : Script automatique (recommand√©)

```bash
# Lancer le script interactif
./start.sh
```

Le script vous permet de choisir :
1. API seulement (FastAPI)
2. Interface seulement (Streamlit)
3. Les deux

### Option 2 : Lancement manuel

```bash
# Terminal 1 - API (avec rechargement automatique)
uv run uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2 - Interface Streamlit
cd streamlit_app
uv run streamlit run app.py
```

L'application sera accessible sur :
- **API** : http://localhost:8000
- **Interface Streamlit** : http://localhost:8501
- **Documentation interactive (Swagger)** : http://localhost:8000/docs
- **Documentation alternative (ReDoc)** : http://localhost:8000/redoc

## Arr√™ter l'Application

### M√©thode rapide

```bash
# Arr√™ter tous les processus uvicorn et streamlit
pkill -9 -f "uvicorn main:app"; pkill -9 -f "streamlit run"
```

### M√©thode par port

```bash
# Tuer tous les processus sur les ports 8000 et 8501
lsof -ti:8000,8501 | xargs kill -9
```

### V√©rifier l'√©tat

```bash
# Voir les processus actifs
ps aux | grep -E "(uvicorn|streamlit)" | grep -v grep

# V√©rifier les ports utilis√©s
lsof -ti:8000,8501
```

## Utilisation de l'API

### Endpoints disponibles

#### 1. Page d'accueil - `GET /`

```bash
curl http://localhost:8000/
```

Retourne les informations de l'API et la liste des endpoints disponibles.

#### 2. Health Check - `GET /health`

```bash
curl http://localhost:8000/health
```

V√©rifie l'√©tat de l'API et de la connexion √† la base de donn√©es.

**R√©ponse** :
```json
{
  "status": "healthy",
  "database": "connected"
}
```

#### 3. Liste des employ√©s - `GET /employees`

R√©cup√®re la liste de tous les employ√©s avec pagination.

```bash
# R√©cup√©rer les 10 premiers employ√©s
curl "http://localhost:8000/employees?limit=10"

# R√©cup√©rer les employ√©s 20 √† 30
curl "http://localhost:8000/employees?skip=20&limit=10"

# R√©cup√©rer tous les employ√©s (max 100 par d√©faut)
curl http://localhost:8000/employees
```

**Param√®tres** :
- `skip` (optionnel) : Nombre d'employ√©s √† ignorer (d√©faut: 0)
- `limit` (optionnel) : Nombre maximum d'employ√©s √† retourner (d√©faut: 100, max: 100)

**R√©ponse** :
```json
{
  "total": 294,
  "employees": [
    {
      "id": 1,
      "genre": "F",
      "age": 24,
      "poste": "Repr√©sentant Commercial",
      "departement": "Commercial",
      "revenu_mensuel": 2033,
      "satisfaction_moyenne": 3.0,
      ...
    }
  ]
}
```

#### 4. Employ√© par ID - `GET /employees/{employee_id}`

R√©cup√®re un employ√© sp√©cifique par son identifiant.

```bash
# R√©cup√©rer l'employ√© avec l'ID 1
curl http://localhost:8000/employees/1

# R√©cup√©rer l'employ√© avec l'ID 42
curl http://localhost:8000/employees/42
```

**R√©ponse (succ√®s)** :
```json
{
  "id": 1,
  "genre": "F",
  "age": 24,
  "statut_marital": "Mari√©(e)",
  "ayant_enfants": "Y",
  "poste": "Repr√©sentant Commercial",
  "domaine_etude": "Infra & Cloud",
  "departement": "Commercial",
  "revenu_mensuel": 2033,
  "satisfaction_moyenne": 3.0,
  "parent_burnout": 0,
  ...
}
```

**R√©ponse (erreur - ID inexistant)** :
```json
{
  "detail": "Employ√© avec l'ID 999 non trouv√©"
}
```

### Exemples d'utilisation avec Python

```python
import requests

# R√©cup√©rer tous les employ√©s
response = requests.get("http://localhost:8000/employees")
data = response.json()
print(f"Total d'employ√©s : {data['total']}")

# R√©cup√©rer un employ√© sp√©cifique
employee = requests.get("http://localhost:8000/employees/1").json()
print(f"Employ√© : {employee['poste']} - {employee['departement']}")

# Calculer la satisfaction moyenne
employees = requests.get("http://localhost:8000/employees?limit=100").json()
avg_satisfaction = sum(e['satisfaction_moyenne'] for e in employees['employees']) / len(employees['employees'])
print(f"Satisfaction moyenne : {avg_satisfaction:.1f}/4")
```

### Exemples d'utilisation avec JavaScript

```javascript
// R√©cup√©rer tous les employ√©s
fetch('http://localhost:8000/employees')
  .then(response => response.json())
  .then(data => console.log(`Total: ${data.total}`));

// R√©cup√©rer un employ√© sp√©cifique
fetch('http://localhost:8000/employees/1')
  .then(response => response.json())
  .then(employee => console.log(employee.poste));
```

## Structure de la Base de Donn√©es

La table `employees` contient **294 employ√©s** avec **34 colonnes** :

### Informations personnelles
- `id` - Identifiant unique
- `genre` - Genre (M/F)
- `age` - √Çge de l'employ√©
- `statut_marital` - Statut marital
- `ayant_enfants` - A des enfants (Y/N)
- `niveau_education` - Niveau d'√©ducation (1-5)

### Informations professionnelles
- `poste` - Intitul√© du poste
- `domaine_etude` - Domaine d'√©tudes
- `departement` - D√©partement (Commercial, Consulting, etc.)
- `niveau_hierarchique_poste` - Niveau hi√©rarchique (1-5)

### Carri√®re et exp√©rience
- `nombre_experiences_precedentes` - Nombre d'emplois pr√©c√©dents
- `annee_experience_totale` - Ann√©es d'exp√©rience totale
- `annees_dans_l_entreprise` - Ann√©es dans l'entreprise actuelle
- `annees_dans_le_poste_actuel` - Ann√©es dans le poste actuel
- `annees_depuis_la_derniere_promotion` - Ann√©es depuis la derni√®re promotion
- `annes_sous_responsable_actuel` - Ann√©es sous le responsable actuel
- `nombre_employee_sous_responsabilite` - Nombre d'employ√©s sous responsabilit√©

### Conditions de travail
- `revenu_mensuel` - Revenu mensuel
- `heure_supplementaires` - Fait des heures suppl√©mentaires (Oui/Non)
- `nombre_heures_travailless` - Nombre d'heures travaill√©es par semaine
- `distance_domicile_travail` - Distance domicile-travail (km)
- `distance_categorie` - Cat√©gorie de distance
- `frequence_deplacement` - Fr√©quence des d√©placements

### Satisfaction et √©valuation
- `satisfaction_employee_environnement` - Satisfaction environnement (1-4)
- `satisfaction_employee_nature_travail` - Satisfaction nature du travail (1-4)
- `satisfaction_employee_equipe` - Satisfaction √©quipe (1-4)
- `satisfaction_employee_equilibre_pro_perso` - Satisfaction √©quilibre vie pro/perso (1-4)
- `satisfaction_moyenne` - Moyenne des satisfactions
- `note_evaluation_precedente` - Note √©valuation pr√©c√©dente (1-4)
- `note_evaluation_actuelle` - Note √©valuation actuelle (1-4)

### Formation et d√©veloppement
- `nb_formations_suivies` - Nombre de formations suivies
- `nombre_participation_pee` - Nombre de participations au PEE

### Indicateurs de risque
- `parent_burnout` - Indicateur burnout parental (0/1)
- `sous_paye_niveau_dept` - Sous-pay√© par rapport au d√©partement (0/1)
- `augementation_salaire_precedente` - Pourcentage d'augmentation pr√©c√©dente

## Interface Streamlit

Une interface web interactive est disponible pour visualiser et explorer les donn√©es de l'API.

### Fonctionnalit√©s

- **üìä Explorer** : Parcourir la liste des employ√©s avec filtres avanc√©s (d√©partement, √¢ge)
- **üîç Recherche** : Rechercher un employ√© par son ID avec affichage d√©taill√©
- **üìà Statistiques** : Visualiser les donn√©es avec des graphiques Plotly interactifs (d√©mographie, r√©mun√©ration, satisfaction)
- **üé® Design moderne** : Th√®me personnalis√© bleu nuit + corail
- **‚ö° Performance** : Cache et optimisations pour une exp√©rience fluide

### Lancement Local

```bash
# Depuis la racine du projet
cd streamlit_app

# Installer les d√©pendances (avec uv)
uv add streamlit pandas plotly requests pytest pytest-cov

# Ou avec pip
pip install -r requirements.txt

# Lancer l'application
uv run streamlit run app.py
# Ou avec streamlit directement
streamlit run app.py
```

L'interface sera accessible sur http://localhost:8501

### D√©ploiement sur Hugging Face Spaces

```bash
# Depuis streamlit_app/
git remote add hf https://huggingface.co/spaces/VOTRE_USERNAME/api-attrition-dashboard
git subtree push --prefix streamlit_app hf main
```

Configuration requise :
- Variable d'environnement `API_URL` dans les settings du Space

### Documentation

- [streamlit_app/README.md](streamlit_app/README.md) : Guide de d√©marrage rapide
- [streamlit_app/DOCUMENTATION.md](streamlit_app/DOCUMENTATION.md) : Documentation p√©dagogique compl√®te
  - Architecture & choix techniques
  - Mise en place technique
  - Tests & qualit√©
  - CI/CD
  - Guide de d√©veloppement

## Prochaines √©tapes

- [ ] Ajouter un endpoint POST pour les pr√©dictions ML
- [ ] Entra√Æner et int√©grer un mod√®le de Machine Learning (pr√©diction d'attrition)
- [ ] Ajouter des filtres avanc√©s sur l'endpoint GET /employees (d√©partement, satisfaction, etc.)
- [ ] Ajouter des endpoints d'analytics et statistiques
- [ ] D√©ploiement sur cloud (Render, Railway, ou Hugging Face Spaces)

## Tests

```bash
pytest
```
